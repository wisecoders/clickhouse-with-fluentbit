# Simplified Kubernetes Logging with Fluentbit and ClickHouse

Logging plays a crucial role in understanding the behavior and performance of applications running in Kubernetes clusters. Leveraging Docker Desktop's Kubernetes functionality, along with Fluentbit and ClickHouse, provides a streamlined approach to logging, offering quick setup and powerful analytics capabilities.

### Why ClickHouse?

ClickHouse is a columnar database management system optimized for high-performance analytics on large volumes of data. Here's why it's an excellent choice for handling logs in Kubernetes environments:

**Scalability:** ClickHouse is horizontally scalable, making it suitable for handling the massive volumes of log data generated by Kubernetes clusters. It can effortlessly handle petabytes of data across distributed nodes, ensuring scalability as your logging needs grow.

**Fast Query Execution:** ClickHouse is designed for rapid query execution, enabling near real-time analysis of log data. Its columnar storage architecture and efficient query processing engine deliver exceptional query performance, allowing for quick insights into application behavior and performance.

**Compression and Storage Efficiency:** ClickHouse offers efficient data compression techniques, reducing storage requirements while maintaining query performance. This enables cost-effective long-term storage of log data, making it an ideal choice for retaining historical logs for compliance and auditing purposes.
**SQL Interface: **ClickHouse provides a familiar SQL interface for querying log data, making it accessible to a wide range of users, including developers, data analysts, and DevOps engineers. Its rich SQL feature set allows for complex analytics, aggregations, and transformations, empowering users to derive meaningful insights from log data with ease.
**Integration with Fluentbit: **ClickHouse seamlessly integrates with Fluentbit, a popular open-source log collector, allowing for efficient ingestion of log data from Kubernetes clusters. Fluentbit can parse, filter, and forward logs to ClickHouse, providing a reliable pipeline for collecting and storing log data in real-time.

##### Initial Setup:
#### What we need:
- Docker Desktop with kubernetes enabled
- kubectl
- kustomize
- Visual Studio Code (you can use your favorite)
- https://infra.app/ - quickly view logs and/or https://github.com/MuhammedKalkan/OpenLens - quickly view definition of services / roles

I'm utilizing Kustomize for deployment purposes. As such, I've developed a quick utility kubezen to facilitate deploying and undeploying deployments. update your manifests directory.

##### We have several components in our deployment setup:

**Alpine Deployment:** This component is a lightweight container designed to assist us in running curl commands and conducting quick network testing. It's particularly useful for tasks like interacting with APIs or testing connectivity within our Kubernetes cluster. We plan to use it specifically for setting up and troubleshooting our ClickHouse deployment.

**Metrics Server:** This component provides valuable insights into resource consumption within our Kubernetes cluster. By monitoring metrics such as memory and CPU usage for each pod, the Metrics Server enables us to optimize resource allocation and identify potential performance bottlenecks.

**ClickHouse:** This is our primary focus and the cornerstone of our deployment. While some may have reservations about deploying stateful sets in Kubernetes due to their inherent complexities, we've decided to embrace this challenge as an opportunity for learning and growth. ClickHouse is a powerful open-source column-oriented database management system capable of handling large volumes of data with exceptional speed and efficiency.

**Fluentbit:** Configuring Fluentbit, a robust and flexible log forwarding and aggregation tool, requires careful attention to detail. Despite its complexity, Fluentbit offers unparalleled capabilities for managing and analyzing log data within Kubernetes environments. We'll guide you through the step-by-step process of setting up Fluentbit to ensure seamless integration with our deployment stack.

**OverlaysAlpine:**
Simplest setup among all. I have deployed it on the default namespace itself.

Refer https://github.com/wisecoders/clickhouse-with-fluentbit/blob/main/overlays/alphine/base/deployment.yaml

Adding kubezen to your bash configuration allows you to execute it from any directory on your machine. When you run kubezen, it looks for the Kubernetes manifests that you've configured in your kubezen path. This flexibility makes it convenient to interact with your Kubernetes cluster and manage resources without having to navigate to specific directories each time.

#### Metrics Server setup:

When setting up the Metrics Server, it's essential to address certain limitations, especially regarding TLS support in Docker Kubernetes environments. Since Docker Kubernetes does not currently support TLS, we need to make adjustments to accommodate this constraint. Specifically, we need to configure the kubelet to allow insecure TLS connections.

To address this issue, we'll need to add the --kubelet-insecure-tls configuration option when deploying the Metrics Server. This setting instructs the Metrics Server to accept insecure TLS connections from the kubelet, ensuring compatibility with Docker Kubernetes environments.

By incorporating this configuration adjustment, we can successfully deploy the Metrics Server in our environment, enabling us to monitor resource utilization and performance metrics within our Kubernetes cluster.

```shell
kubezen up kube-metrics
```

**ClickHouse installation and setup:**
Installing ClickHouse is relatively straightforward, especially with Docker simplifying certain aspects such as storage management. Docker's provision of a storage class called hostPath alleviates much of the complexity typically associated with configuring storage for stateful applications like ClickHouse in Kubernetes. With hostPath, we can leverage the local storage of the host machine directly, streamlining the setup process and enabling us to focus on other aspects of the deployment.
```shell
kubezen up clickhouse
```
we can see clickhouse is up and running.

To validate if ClickHouse is working, you can leverage the Alpine container to interact with ClickHouse from within your Kubernetes cluster. Here's a general outline of the steps you might take:

Access the Alpine container: Use kubectl exec to access the Alpine container running in your Kubernetes cluster. For example:
```shell
kubectl exec -it <alpine-pod-name> - /bin/sh
```

Also, we can login to clickhouse container and use clickhouse client to validate
Let's create table to receive logs from FluentBit(Reference)

```sql
CREATE DATABASE fluentbit
SET allow_experimental_object_type = 1

CREATE TABLE fluentbit.kube
(
    timestamp DateTime,
    log JSON,
    host LowCardinality(String),
    pod_name LowCardinality(String)
)
Engine = MergeTree ORDER BY tuple(host, pod_name, timestamp)

```
#### FluentBit installation and setup

FluentBit is the most difficult process among all of them. But I have covered all of them.

**ConfigMap metadata:** Specifies the metadata for the ConfigMap, including its name (fluent-bit-config) and namespace (kube-monitoring).
fluent-bit.conf: This section contains the main configuration settings for Fluent Bit:

**Daemon Off:** Specifies that Fluent Bit should run in the foreground.
Flush 5: Sets the flush interval to 5 seconds.
Log_Level info: Sets the logging level to info.
Parsers_File: Specifies the location of parser configuration files.
HTTP_Server On: Enables the HTTP server for Fluent Bit.
HTTP_Listen 0.0.0.0: Specifies the IP address to listen on for HTTP requests.
HTTP_Port 2020: Specifies the port for the HTTP server.
Health_Check On: Enables health checks for Fluent Bit.

**input-kubernetes.conf:** Configures input plugins for Fluent Bit to collect logs from Kubernetes:
tail: Collects logs from log files in Kubernetes containers.
systemd: Collects logs from systemd journal, specifically from the kubelet service.

**filters.conf:** Configures filters for processing logs before sending them to ClickHouse:
**kubernetes:** Parses Kubernetes logs and extracts relevant metadata.

nest: Groups log fields under a common parent field named log.

lua: Executes a Lua script to manipulate log records. In this case, it sets specific fields such as host and pod_name.


**outputs.conf:** Specifies the output plugin for Fluent Bit to send logs to ClickHouse:
http: Sends logs to ClickHouse via HTTP.

tls off: Disables TLS for simplicity.

Host: Specifies the ClickHouse service hostname.

port: Specifies the ClickHouse service port.

URI: Specifies the ClickHouse endpoint for inserting logs.

format json_stream: Sets the log format to JSON.

json_date_key and json_date_format: Specifies the timestamp format for log records.

6. functions.lua: Defines a Lua function called set_fields to manipulate log records before sending them to ClickHouse. In this case, it extracts and modifies specific fields such as host and pod_name.

Overall, this configuration sets up Fluent Bit to collect Kubernetes logs, parse them, apply filters, and then send them to ClickHouse for storage and analysis. It also includes some custom logic to manipulate log records before sending them to ClickHouse.

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: fluent-bit-config
  namespace: kube-monitoring
data:
  fluent-bit.conf: |
    [SERVICE]
        Daemon Off
        Flush 5
        Log_Level info
        Parsers_File /fluent-bit/etc/parsers.conf
        Parsers_File /fluent-bit/etc/conf/custom_parsers.conf
        HTTP_Server On
        HTTP_Listen 0.0.0.0
        HTTP_Port 2020
        Health_Check On

    @INCLUDE input-kubernetes.conf
    @INCLUDE filters.conf
    @INCLUDE outputs.conf

  input-kubernetes.conf: |
    [INPUT]
        Name tail
        Path /var/log/containers/*.log
        multiline.parser docker, cri
        Tag kube.*
        Mem_Buf_Limit 5MB
        Skip_Long_Lines On

    [INPUT]
        Name systemd
        Tag host.*
        Systemd_Filter _SYSTEMD_UNIT=kubelet.service
        Read_From_Tail On
  filters.conf: |
    [FILTER]
        Name kubernetes
        Match kube.*
        Merge_Log On
        Keep_Log Off
        K8S-Logging.Parser On
        K8S-Logging.Exclude On

    [FILTER]
        Name nest
        Match *
        Operation nest
        Wildcard *
        Nest_under log   

    [FILTER]
        Name lua
        Match *
        Script /fluent-bit/scripts/functions.lua
        Call set_fields

  outputs.conf: |
    [OUTPUT]
        Name http
        tls off
        Match *        
        Host clickhouse-service.default.svc.cluster.local
        port 8123
        URI /?query=INSERT+INTO+fluentbit.kube+FORMAT+JSONEachRow
        format json_stream
        json_date_key timestamp
        json_date_format epoch    

  functions.lua: |
    function set_fields(tag, timestamp, record)
      record['host'] = record['log']['kubernetes']['host']
      record['log']['kubernetes']['host'] = nil
      record['pod_name'] = record['log']['kubernetes']['pod_name']
      record['log']['kubernetes']['pod_name'] = nil
      return 2, timestamp, record
    end  
```

Let's deploy

```shell
kubezen up clickhouse
```
Let's view the logs to make sure the logs getting published to clickhouse:
Yes, it is working. Let's get back to ClickHouse and check our logs.

Let's write some aggregate query:

clickhouse-0.clickhouse.default.svc.cluster.local :) SELECT
    count(),
    namespace
FROM fluentbit.kube
GROUP BY log.kubernetes.namespace_name AS namespace

SELECT
    count(),
    namespace
FROM fluentbit.kube
GROUP BY log.kubernetes.namespace_name AS namespace

Query id: b47ab138-fb5c-425f-bd0b-06849c2b7e36

   ┌─count()─┬─namespace───────┐
1. │    8520 │ default         │
2. │    4283 │ kube-system     │
3. │    9877 │ kube-monitoring │
   
   └─────────┴─────────────────┘

3 rows in set. Elapsed: 0.016 sec. Processed 22.68 thousand rows, 459.03 KB (1.40 million rows/s., 28.31 MB/s.)
Peak memory usage: 120.59 KiB.

That's an impressive performance from ClickHouse! Processing 1.4 million rows in just 0.016 seconds showcases its exceptional speed and efficiency in handling large volumes of data. ClickHouse's column-oriented architecture and highly optimized query execution engine enable it to achieve such remarkable performance, making it a preferred choice for data analytics and real-time processing tasks. With ClickHouse, you can confidently manage and analyze vast amounts of data with minimal latency, empowering your analytics workflows to deliver insights rapidly and efficiently.
and here is memory usage.

With memory usage at 1.93GB and CPU usage at 4%, ClickHouse continues to demonstrate efficient resource utilization. Despite the lower memory usage compared to the previous estimation, ClickHouse is still effectively managing its memory footprint while processing data. The modest CPU usage suggests that ClickHouse is efficiently executing queries without putting significant strain on the CPU.

Overall, these metrics confirm ClickHouse's ability to deliver high performance while maintaining efficient resource utilization, contributing to a smooth and responsive data processing experience.

ClickHouse reigns supreme. Its lightning-fast performance, handling 1.4 million rows in a mere 0.016 seconds, is nothing short of revolutionary. But its efficiency doesn't stop there - with memory usage capped at 1.93GB and CPU usage a mere 4%, ClickHouse sets the standard for resource optimization. By harnessing the power of ClickHouse, organizations can unlock the full potential of their data, paving the way for faster insights, smarter decisions, and a brighter future.

All the source code is available on GitHub.

https://github.com/wisecoders/clickhouse-with-fluentbit

My next article will delve into the seamless integration of ClickHouse with Application Lifecycle Management tools like Grafana and Uptrace. Stay tuned for insights on how this integration can revolutionize data visualization and monitoring, providing invaluable insights into system performance. And with that, I'll call it a wrap.
